'mdconfig', 'status-notification.initial-context-factory', 'Active MQ Initial Context Factory', 'MQ notification factory for status notification',  1, 'org.apache.activemq.jndi.ActiveMQInitialContextFactory', 'text', 1
'mdconfig', 'status-notification.provider-url', 'Notification Provider URL', 'MQ server address for status notification',  1, 'tcp://localhost:61616', 'text', 1
'mdconfig', 'status-notification.halt-queue', 'Halt Queue', 'Halt Queue info for status notification',  1, 'Success', 'text', 1
'mdconfig', 'status-notification.term-queue', 'Term Queue', 'Term Queue info for status notification',  1, 'Failure', 'text', 1
'mdconfig', 'deploy.script-path', 'Deployment script path', 'Deployment script path',  1, '/home/${bdre_user_name}/bdre/bdre-scripts/deployment', 'text', 1
'mdconfig', 'execute.log-path', 'Log File Location', 'Path for log file',  1, '/tmp/logfile-', 'text', 1
'mdconfig', 'execute.script-path', 'Process Execute Script Path', 'Execute workflow script path',  1, '/home/${bdre_user_name}/bdre/bdre-scripts/execution', 'text', 1
'mdconfig', 'upload.base-directory', 'Base Directory For Upload', 'Base Directory where you want to upload jars and hqls',  1, '/home/${bdre_user_name}/bdre-wfd', 'text', 1
'mdconfig', 'r.shell-script', 'Location of R shell script', 'Location of R shell script', 1, 'Rhadoop.sh', 'text', 1
'imconfig', 'common.default-fs-name', 'Default file system name','Default file system name', 1, 'hdfs://${name_node_hostname}:${name_node_port}', 'text', 1
'imconfig', 'etl.hive-connection', 'URL of Hive connection','URL of Hive connection', 1, 'jdbc:hive2://${hive_server_hostname}:10000', 'text', 1
'imconfig', 'etl.hive-jdbcuser', 'Hive JDBC User','Hive JDBC User', 1, '${hive_jdbc_user}', 'text', 1
'imconfig', 'etl.hive-jdbcpassword', 'Hive JDBC Pwd','Hive JDBC Pwd', 1, '${hive_jdbc_password}', 'text', 1
'imconfig', 'etl.hdfs-raw-directory', 'Hadoop raw directory','Hadoop raw directory' , 1, '/raw', 'text', 1
'imconfig', 'etl.hive-metastore-uris', 'URL of Hive metastore','URL of Hive metastore', 1, 'thrift://${thrift_hostname}:9083', 'text', 1
'imconfig', 'etl.local-download-directory', 'Directory to store downloaded files','Directory to store downloaded files', 1, '/home/user/Downloads', 'text', 1
'imconfig', 'file-mon.thread-wait', 'Thread waiting time','Thread waiting time', 1, '500', 'text', 1
'imconfig', 'file-mon.dirs', 'Monitoring files directory list','Monitoring files directory list', 1, '/home/${edge_node_user_name}/oozies,/home/${edge_node_user_name}/datasets', 'text', 1
'imconfig', 'file-mon.filter', 'Filter Information','Filter Information', 1, 'merchant\\s[0-9],test\\s[0-9]', 'text', 1
'imconfig', 'file-mon.sub-processIds', 'Sub process IDs','Sub process IDs', 1, '1,2', 'text', 1
'imconfig', 'file-mon.serverIds', 'Server IDs','Server IDs', 1, '100,200', 'text', 1
'imconfig', 'dq.drools-url-prefix', 'URL prefix for drools','URL prefix for drools', 1, 'http://${drools_hostname}:8080/guvnor56/rest/packages/', 'text', 1
'imconfig', 'data-import.hadoop-home', 'Hadoop home directory for exporting','Hadoop home directory for exporting', 1, '/usr/lib/hadoop', 'text', 1
'imconfig', 'data-import.target-dir', 'Target direcotry to store imported data','Target direcotry to store imported data', 1, 'hdfs://${name_node_hostname}:${name_node_port}/user/${edge_node_user_name}/import', 'text', 1
'imconfig', 'data-import.jar-output-dir-oozie', 'Jar output directory for importing - oozie','Jar output directory for importing', 1, '/tmp/bdre-oozie/sqoop/jar-output', 'text', 1
'imconfig', 'data-import.jar-output-dir-airflow', 'Jar output directory for importing - airflow','Jar output directory for importing', 1, '/tmp/bdre-airflow/sqoop/jar-output', 'text', 1
'imconfig', 'mq-import.file-size-units', 'File sizes unit','File sizes unit', 1, 'KB', 'text', 1
'imconfig', 'mq-import.rotation-file-size', 'Rotation file size','Rotation file size', 1, '10', 'text', 1
'imconfig', 'mq-import.target-directory', 'Directory to import MQ file','Directory to import MQ file', 1, 'hdfs://${name_node_hostname}:${name_node_port}/user/${edge_node_user_name}/mq-import', 'text', 1
'imconfig', 'data-export.hadoop-home', 'Hadoop home directory for exporting','Hadoop home directory for exporting', 1, '/usr/lib/hadoop', 'text', 1
'imconfig', 'data-export.jar-output-dir', 'Jar output directory for exporting','Jar output directory for exporting', 1, '/tmp/bdre/sqoop/jar-output', 'text', 1
'imconfig', 'eventing.metadata-broker-list', 'Metadata Brokers URL','Metadata Brokers URL', 1, 'localhost://:9092', 'text', 1
'imconfig', 'hadoop-conf-dir', 'Hadoop Configuration Directory','Hadoop Configuration Directory', 1, '/etc/hadoop/conf', 'text', 1
'imconfig', 'kerberos-user-name', 'Kerberos UserName','Kerberos UserName', 1, '${kerberos_username}', 'text', 1
'imconfig', 'kerberos-keytab-file-location', 'Kerberos keytab file location','Kerberos keytab file location', 1, '${kerberos_keytab_file_location}', 'text', 1
#For deploy and execution scripts
'scripts_config', 'bdreLinuxUserName', '-','-' , 1, '${bdre_user_name}', 'text' , 1
'scripts_config', 'nameNodeHostName', '-','-' , 1, '${name_node_hostname}', 'text' , 1
'scripts_config', 'nameNodePort', '-','-' , 1, '${name_node_port}', 'text' , 1
'scripts_config', 'jobTrackerHostName', '-','-' , 1, '${thrift_hostname}', 'text' , 1
#For cloudera this is 8032
'scripts_config', 'jobTrackerPort', '-','-' , 1, '${job_tracker_port}', 'text' , 1
'scripts_config', 'hiveConfDir', '-','-' , 1, 'conf', 'text' , 1
'scripts_config', 'flumeLibDir', '-','-' , 1, '${flume_path}', 'text' , 1
'scripts_config', 'uploadBaseDir', '-','-' , 1, '~/bdre-wfd', 'text' , 1
'scripts_config', 'bdreVersion', '-','-' , 1, '1.1-SNAPSHOT', 'text' , 1
'scripts_config', 'oozieHost', '-','-' , 1, '${oozie_host}', 'text' , 1
'scripts_config', 'ooziePort', '-','-' , 1, '${oozie_port}', 'text' , 1
'scripts_config', 'edgeNodeHostName', '-','-' , 1, '${edge_node_host_name}', 'text' , 1
'scripts_config', 'edgeNodeUserName', '-','-' , 1, '${edge_node_user_name}', 'text' , 1
'scripts_config', 'edgeNodePassword', '-','-' , 1, '${edge_node_password}', 'text' , 1
'scripts_config', 'oozieUrl', '-','-' , 1, '${oozie_url}', 'text' , 1
'scripts_config', 'logDir', '-','-' , 1, '/var/log/BDRE', 'text' , 1
'scripts_config', 'repoName', '-','-' , 1, 'origin', 'text' , 1
'scripts_config', 'branchName', '-','-' , 1, 'develop', 'text' , 1
'scripts_config', 'gitURL', '-','-' , 1, 'https://github.com/WiproOpenSourcePractice/bdreappstore.git', 'text' , 1
'scripts_config', 'airflowInstallDir', '-','Airflow Installation directory upto bin folder' , 1, '${airflow_install_dir}', 'text' , 1
'scripts_config', 'airflowDagPath', '-','-' , 1, '/home/${bdre_user_name}/airflow/dags', 'text' , 1

'file_format', 'Delimited', 'DELIMITED','delimited file' , 1, 'Delimited', 'text' , 1
'file_format', 'XML', 'XML','XML file' , 1, 'XML', 'text' , 1
'file_format', 'Json', 'JSON','JSON file' , 1, 'Json', 'text' , 1
'file_format', 'Regex', 'REGEX','REGEX file' , 1, 'Regex', 'text' , 1

'ApacheLog', 'ipAddress', 'ipAddress','ipAddress' , 1, 'ipAddress', 'String' , 1
'ApacheLog', 'clientIdentd', 'clientIdentd','clientIdentd' , 1, 'clientIdentd', 'String' , 1
'ApacheLog', 'userID', 'userID','userID' , 1, 'userID', 'String' , 1
'ApacheLog', 'dateTimeString', 'dateTimeString','dateTimeString' , 1, 'dateTimeString', 'String' , 1
'ApacheLog', 'method', 'method','method' , 1, 'method', 'String' , 1
'ApacheLog', 'endpoint', 'endpoint','endpoint' , 1, 'endpoint', 'String' , 1
'ApacheLog', 'protocol', 'protocol','protocol' , 1, 'protocol', 'String' , 1
'ApacheLog', 'responseCode', 'responseCode','responseCode' , 1, 'responseCode', 'String' , 1
'ApacheLog', 'contentSize', 'contentSize','contentSize' , 1, 'contentSize', 'String' , 1


'41','param','Parameters','Additional parameters', 1,'true','text', 1
'43','default','Filter','Filter', 1,'true','filter', 1
'60','default','Take','Take', 1,'true','take', 1
'61','default','Persist','Persist', 1,'true','persist', 1
'62','default','Repartition','Repartition', 1,'true','repartition', 1
'62','default','Window','Window', 1,'true','window', 1
'66','default','Sort','Sort', 1,'true','sort', 1

'42','record','Kafka Properties','Kafka', 1,'true','source', 1
'46','record','Twitter Properties','Twitter', 1,'true','source', 1
'47','record','Kinesis Properties','Kinesis', 1,'true','source', 1
'48','record','MQTT Properties','MQTT', 1,'true','source', 1
'49','record','RabbitMQ Properties','RabbitMQ', 1,'true','source', 1
'50','record','Socket Properties','Socket', 1,'true','source', 1
'51','record','S3 Properties','S3', 1,'true','source', 1

'52','connection','Kafka Properties','Kafka', 1,'true','emitter', 1
'53','connection','Kinesis Properties','Kinesis', 1,'true','emitter', 1
'54','connection','MQTT Properties','MQTT', 1,'true','emitter', 1
'55','connection','RabbitMQ Properties','RabbitMQ', 1,'true','emitter', 1
'56','connection','Socket Properties','Socket', 1,'true','emitter', 1
'57','connection','JDBC Properties','JDBC', 1,'true','emitter', 1

'44','connection','HDFS Properties','hdfs',1,'true','persistentStore',1
'58','connection','Hive Properties','Hive',1,'true','persistentStore',1
'59','connection','HBase Properties','HBase',1,'true','persistentStore',1

'column_transformation','no transformation','no transformation','no column transformation',1,'no transformation','text',1
'column_transformation','upper','upper','changing column data in uppercase',1,'upper','text',1
'column_transformation','lower','lower','changing column data in lowercase',1,'lower','text',1
'column_transformation','round','round','It returns the rounded BIGINT value of the double',1,'round','text',1
'column_transformation','trim','trim','It returns the string resulting from trimming spaces from both ends',1,'trim','text',1
'column_transformation','floor','floor','It returns the maximum BIGINT value that is equal or less than the double',1,'floor','text',1

'Source_Connection_Type','Kafka','Kafka','', 0,'Kafka','text', 1
'Source_Connection_Type','Twitter','Twitter','', 0, 'Twitter','text', 1
'Source_Connection_Type','Kinesis','Kinesis','', 0, 'Kinesis','text', 1
'Source_Connection_Type','MQTT','MQTT','', 0, 'MQTT','text', 1
'Source_Connection_Type','RabbitMQ','RabbitMQ','', 0, 'RabbitMQ','text', 1
'Source_Connection_Type','Socket','Socket','', 0, 'Socket','text', 1
'Source_Connection_Type','S3','S3','', 0, 'S3','text', 1

'Kafka_Source_Connection','zookeeper.connect','Zookeeper URL','', 1,'','text', 1
'Kafka_Source_Connection','bootstrap.servers','List of Kafka Brokers','', 1,'','text', 1
'Kafka_Source_Connection','topicName','Topic Name','config- Topicname', 1,'','text', 1

'Twitter_Source_Connection','consumerKey','Consumer Key','OAuth consumer key', 1,'','password', 1
'Twitter_Source_Connection','consumerSecret','Consumer Secret','OAuth consumer secret', 1,'','password', 1
'Twitter_Source_Connection','accessToken','Access Token','OAuth access token', 1,'','password', 1
'Twitter_Source_Connection','accessTokenSecret','Access Token Secret','OAuth toekn secret', 1,'','password', 1
'Twitter_Source_Connection','keywords','HASHTAGS','config - keywords based on which you want filter your tweets', 1,'#BigData','text', 1

'Kinesis_Source_Connection','appName','App Name','', 1,'','text', 1
'Kinesis_Source_Connection','streamName','Stream Name','config- Name of kenisis stream', 1,'','text', 1
'Kinesis_Source_Connection','endPointURL','End Point URL','', 1,'config- ','text', 1
'Kinesis_Source_Connection','regionName','Region Name','config- ', 1,'','text', 1
'Kinesis_Source_Connection','initialPosition','Initial Position','config- ', 1,'','text', 1
'Kinesis_Source_Connection','checkpointInterval','Checkpoint Interval','config- ', 1,'','text', 1
'Kinesis_Source_Connection','storageLevel','Storage Level','config- ', 1,'','text', 1

'MQTT_Source_Connection','brokerUrl','Broker Url','', 1,'','text', 1
'MQTT_Source_Connection','storageLevel','Storage Level','', 1,'','text', 1
'MQTT_Source_Connection','topicName','Topic Name','config-', 1,'','text', 1

'RabbitMQ_Source_Connection','host','Host','', 1,'','text', 1
'RabbitMQ_Source_Connection','username','Username','', 1,'','text', 1
'RabbitMQ_Source_Connection','password','Password','', 1,'','text', 1
'RabbitMQ_Source_Connection','exchangeName','Exchange Name','config- Name of the RabbitMQ exchange.', 1,'','text', 1
'RabbitMQ_Source_Connection','exchangeType','Exchange Type','config- Exchange types: DIRECT, TOPIC & FANOUT.', 1,'','text', 1
'RabbitMQ_Source_Connection','exchangeDurable','Exchange Durable','config- TRUE for a durable exchange.', 1,'','text', 1
'RabbitMQ_Source_Connection','exchangeAutoDelete','Exchange Auto Delete','config- TRUE to enable auto-delete.', 1,'','text', 1
'RabbitMQ_Source_Connection','routingKey','Routing Key','config- Routing key that binds an exchange with a queue.', 1,'','text', 1
'RabbitMQ_Source_Connection','queueName','Queue Name','config- Name of the RabbitMQ queue.', 1,'','text', 1
'RabbitMQ_Source_Connection','queueDurable','Queue Durable','config- TRUE for a durable queue.', 1,'','text', 1
'RabbitMQ_Source_Connection','queueAutoDelete','Queue Auto Delete','config- TRUE to enable auto-delete.', 1,'','text', 1

'Socket_Source_Connection','host','Host','IP address of the machine where Socket is running', 1,'','text', 1
'Socket_Source_Connection','port','Port','Port of the machine where Socket is running', 1,'','text', 1

'S3_Source_Connection','awsKeyId','AWS KeyId','', 1,'','text', 1
'S3_Source_Connection','secretAccesKey','Secret Acces Key','', 1,'','text', 1
'S3_Source_Connection','bucketName','Bucket Name','config- ', 1,'','text', 1
'S3_Source_Connection','path','Path','config- ', 1,'','text', 1

'Emitter_Connection_Type','kafka','Kafka','', 0,'Kafka','text', 1
'Emitter_Connection_Type','jdbc','JDBC','', 0,'JDBC','text', 1
'Emitter_Connection_Type','Kinesis','Kinesis','', 0, 'Kinesis','text', 1
'Emitter_Connection_Type','MQTT','MQTT','', 0, 'MQTT','text', 1
'Emitter_Connection_Type','RabbitMQ','RabbitMQ','', 0, 'RabbitMQ','text', 1
'Emitter_Connection_Type','socket','Socket','', 0,'Socket','text', 1

'Kafka_Emitter_Connection','topicName','Topic Name','', 1,'','text', 1
'Kafka_Emitter_Connection','zookeeper.connect','Zookeeper URL','', 1,'','text', 1
'Kafka_Emitter_Connection','bootstrap.servers','List of Kafka Brokers','', 1,'','text', 1

'Kinesis_Emitter_Connection','appName','App Name','', 1,'','text', 1
'Kinesis_Emitter_Connection','streamName','Stream Name','config- Name of kenisis stream', 1,'','text', 1
'Kinesis_Emitter_Connection','endPointURL','End Point URL','', 1,'config- ','text', 1
'Kinesis_Emitter_Connection','regionName','Region Name','config- ', 1,'','text', 1
'Kinesis_Emitter_Connection','initialPosition','Initial Position','config- ', 1,'','text', 1
'Kinesis_Emitter_Connection','checkpointInterval','Checkpoint Interval','config- ', 1,'','text', 1
'Kinesis_Emitter_Connection','storageLevel','Storage Level','config- ', 1,'','text', 1

'MQTT_Emitter_Connection','brokerUrl','Broker Url','', 1,'','text', 1
'MQTT_Emitter_Connection','storageLevel','Storage Level','', 1,'','text', 1
'MQTT_Emitter_Connection','topicName','Topic Name','config-', 1,'','text', 1

'RabbitMQ_Emitter_Connection','host','Host','', 1,'','text', 1
'RabbitMQ_Emitter_Connection','username','Username','', 1,'','text', 1
'RabbitMQ_Emitter_Connection','password','Password','', 1,'','text', 1
'RabbitMQ_Emitter_Connection','exchangeName','Exchange Name','config- Name of the RabbitMQ exchange.', 1,'','text', 1
'RabbitMQ_Emitter_Connection','exchangeType','Exchange Type','config- Exchange types: DIRECT, TOPIC & FANOUT.', 1,'','text', 1
'RabbitMQ_Emitter_Connection','exchangeDurable','Exchange Durable','config- TRUE for a durable exchange.', 1,'','text', 1
'RabbitMQ_Emitter_Connection','exchangeAutoDelete','Exchange Auto Delete','config- TRUE to enable auto-delete.', 1,'','text', 1
'RabbitMQ_Emitter_Connection','routingKey','Routing Key','config- Routing key that binds an exchange with a queue.', 1,'','text', 1
'RabbitMQ_Emitter_Connection','queueName','Queue Name','config- Name of the RabbitMQ queue.', 1,'','text', 1
'RabbitMQ_Emitter_Connection','queueDurable','Queue Durable','config- TRUE for a durable queue.', 1,'','text', 1
'RabbitMQ_Emitter_Connection','queueAutoDelete','Queue Auto Delete','config- TRUE to enable auto-delete.', 1,'','text', 1

'Socket_Emitter_Connection','host','Host','IP address of the machine where Socket is running', 1,'','text', 1
'Socket_Emitter_Connection','port','Port','Port of the machine where Socket is running', 1,'','text', 1

'JDBC_Emitter_Connection','jdbcDriver','JDBC Driver','', 1,'','text', 1
'JDBC_Emitter_Connection','jdbcUrl','JDBC URL','', 1,'','text', 1
'JDBC_Emitter_Connection','username','Username','', 1,'','text', 1
'JDBC_Emitter_Connection','password','Password','', 1,'','text', 1
'JDBC_Emitter_Connection','schema','Schema','', 1,'','text', 1
'JDBC_Emitter_Connection','table','Table Name','config- ', 1,'','text', 1

'PersistentStores_Connection_Type','hdfs','HDFS','', 0,'HDFS','text', 1
'PersistentStores_Connection_Type','hive','Hive','', 0,'Hive','text', 1
'PersistentStores_Connection_Type','hbase','HBase','', 0,'HBase','text', 1

'HDFS_PersistentStores_Connection','nameNodeHost','Name Node Host','', 1,'','text', 1
'HDFS_PersistentStores_Connection','nameNodePort','Name Node Port','', 1,'','text', 1
'HDFS_PersistentStores_Connection','hdfsPath','HDFS PATH','', 1,'','text', 1

'Hive_PersistentStores_Connection','connectionUrl','Connection Url','', 1,'','text', 1
'Hive_PersistentStores_Connection','dbName','Database Name','', 1,'','text', 1
'Hive_PersistentStores_Connection','username','Username','', 1,'','text', 1
'Hive_PersistentStores_Connection','password','Password','', 1,'','text', 1

'HBase_PersistentStores_Connection','hdfsUser','HDFS User','HDFS user name.', 1,'','text', 1
'HBase_PersistentStores_Connection','zKHost','Zookeeper Host','Zookeeper host name for hbase cluster.', 1,'','text', 1
'HBase_PersistentStores_Connection','zK Port','Zookeeper Port','Zookeeper port for hbase cluster.', 1,'','text', 1
'HBase_PersistentStores_Connection','Encoding','Encoding','config- ', 1,'','text', 1
'HBase_PersistentStores_Connection','connectionRetries','Connection Retries','config- ', 1,'','text', 1




